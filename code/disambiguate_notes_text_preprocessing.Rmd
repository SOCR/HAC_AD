---
title: "Health Analytics Collaboratory (HAC) - University of Michigan"
author: "<h3>Ivo Dinov, Cooper Stansbury, Haiyin Liu, Bingxin Chen</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
output:
  html_document:
    highlight: tango
    includes:
      before_body: SOCR_header.html
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
  word_document:
    toc: yes
    toc_depth: '2'
tags:
- HAC
- DSPA
- SOCR
- MIDAS
- Big Data
- Predictive Analytics
- Computable Phenotypes
subtitle: '<h2><u>HAC Alzheimer''s Disease Digital Health Analytics Project: Text
  Preprocessing </u></h2>'
---

# Processing Logistics

## Note on Project Directory  Structure:
This project assumes that:
  - There is a root working directory (in my case, called 'HAC')
  - There is a subdirectory called `data/` with the data files (`.xlsx`).
  - `CARD_dataset_tools/` has been downloaded to the working directory.
  - `.Rmd` scripts are in a directory called something like `notebooks/`.
 
## Working Citations
  - [Text_Processing_In_R](http://www.mjdenny.com/Text_Processing_In_R.html)
  - [r-text-mining-pre-processing](https://analytics4all.org/2016/12/22/r-text-mining-pre-processing/)
  - [SICSS_Basic_Text_Analysis](https://cbail.github.io/SICSS_Basic_Text_Analysis.html)
  - [19_NLP_TextMining](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/19_NLP_TextMining.html)

## Packages/Dependencies

```{r message=FALSE}
library(tidyverse)
library(tidytext)
library(topicmodels)
library(tm)
library(SnowballC)
library(tidytext)
library(tidyr)
library(dplyr)
library(reshape2)
library(knitr)
library(kableExtra)
library(stringi)
library(formattable)
library(wordcloud2)
library(RColorBrewer)
library(stringdist)

## another text_processing library that may be worth considering, spaCy is extremely powerful
## https://github.com/quanteda/spacyr
# library(quanteda)
```

Selected Package Documentation Links
  - [library(tm)](https://cran.r-project.org/web/packages/tm/tm.pdf)
  - [library(SnowballC)](https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf)
  - [library(tidytext)](https://cran.r-project.org/web/packages/tidytext/tidytext.pdf)

```{r}
sessionInfo()
```

```{r}
# expect 1.0.1, 0.7.0, 1.0.1
sapply(c('repr', 'IRdisplay', 'IRkernel'), function(p) paste(packageVersion(p)))
```

## Source Custom Functions
Source the `load_and_convert_functions.R` from the local file.

```{r}
source('code/load_and_convert_functions.R')
```

# Data Dictionary 
 Overview of the data:

```{r, echo=FALSE, results='asis'}
data_dictionary_path <- 'data/Data_Dictionary.xlsx' 
header <- readxl::read_excel(data_dictionary_path, sheet = "Notes", range="A1:B4", trim_ws = TRUE)

knitr::kable(header) %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F)
```


```{r, echo=FALSE, results='asis'}
enc_data_dictionary <- readxl::read_excel(data_dictionary_path, 
                              sheet = "Notes", 
                              col_types = c("text", "text", "text", "text"),
                              col_names=TRUE, 
                              range="A6:D18", 
                              trim_ws = TRUE)

knitr::kable(enc_data_dictionary) %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F)
```

# Load data
Load the data from the source file. Assumes file in `.xlsx` format. 

```{r}
getwd() # make sure you're in the right spot! Expect "........./HAC_AD"
```

## Sample Data 

This assumes that there is a separate directory for the data file. **Watch out for paths.**
```{r}
relative_sample_path <- "data/DS_PreprocessedAnonymized_Text.xlsx" 
```

## Full Data Set

**WARNING:** the following file is magnitudes larger than the others and requires a long time (and a lot of memory) to load. Proceed with caution.

```{r}
# notes_path <- 'data/full_set/Notes.dsv'
# notes_path <- "data/full_set/TEST_Notes.dsv"
```

### Investigating File Irregularities

**NOTE:** The parameters on this function affect the total number of rows in the resulting `data.frame`. The following note is from the data office:

  _"The text fields have been cleaned up so that carriage return/line feed and tab characters have been converted into a space..... I noticed that a few slipped through, however,      so that you will occasionally see two spaces together."_

```{bash}
# wc -l "data/full_set/Notes.dsv"
```

Something breaks down in the middle of this file. At the beginnning, each record is on it's own line, but at the end this is no longer the case. 

```{bash}
# head -n 3 "data/full_set/Notes.dsv"
```

After some manual searching, we can see that this is the first section of the file that breaks. Lines 178791-178793 should be a single record (line), but they are not.

```{bash}
# sed -n "178790,178794p" "data/full_set/Notes.dsv"
```

```{bash}
# sed -n "178793,178805p" "data/full_set/Notes.dsv"
```

It's bad further on as well.

```{bash}
# sed -n "278793,278805p" "data/full_set/Notes.dsv"
```

```{bash}
# tail -n 200 "data/full_set/Notes.dsv"
```


```{bash}
# head -n 178789 "data/full_set/Notes.dsv" > "data/full_set/TEST_Notes.dsv"
```

```{r}
# Temporarily set the path to a testing file (the records suspected to be correct...)
# notes_path <- "data/full_set/TEST_Notes.dsv"
```

```{r}
# Read the full data 
# source('code/load_and_convert_functions.R')
# raw_data_df <- read_notes_dsv(filepath = notes_path)
# head(raw_data_df)
```

Reading sample data (100 de-id cases).
```{r}
raw_data_df <- read_xlsx_from_path(relative_sample_path)
names(raw_data_df)[names(raw_data_df) == 'DS_PreprocessedAnonymized_Text'] <- 'TEXT' # make names consistent 
raw_data_df <- raw_data_df[1:(length(raw_data_df)-1)] # drop last column
sampled_df <- raw_data_df # rename for consistency
head(sampled_df)
```


```{r}
dim(sampled_df)
```

### Sampling from Full Data Set
Randomly sample from the full set to speed up development while maximizing exposure to corner cases.

```{r}
# sampled_df <- raw_data_df[sample(nrow(raw_data_df), 1000), ]
# head(sampled_df)
```


### Data Type Transformations

```{r}
names(sampled_df)
```


```{r}
str(sampled_df)
```

```{r}
transformed_df <- transform(sampled_df, 
                    SOURCE_DATABASE = as.factor(SOURCE_DATABASE),
                    NOTE_TYPE = as.factor(NOTE_TYPE),
                    DATE_OF_SERVICE = as.Date(DATE_OF_SERVICE, format='%m-%d-%Y %H:%M'),
                    CREATE_DATETIME = as.Date(CREATE_DATETIME, format='%m-%d-%Y %H:%M'),
                    AUTHOR_SERVICE = as.factor(AUTHOR_SERVICE), 
                    DEPT_ABBR = as.factor(DEPT_ABBR), 
                    DEPT_NAME = as.factor(DEPT_NAME)) 

str(transformed_df)
```

### Force character encoding

```{r}
transformed_df$TEXT <- stri_encode(transformed_df$TEXT, "", "UTF-8")
text_df <- transformed_df # rename for consistency
# head(text_df , 2)
```


# CARD Abbreviation Disambiguiation

We utilize on [CARD](https://sbmi.uth.edu/ccb/resources/abbreviation.htm):
  - Wu Y, Denny JC, Trent Rosenbloom S, Miller RA, Giuse DA, Wang L, et al. [A long journey to short abbreviations: developing an open-source framework for clinical abbreviation recognition and disambiguation (CARD)](https://www.ncbi.nlm.nih.gov/pubmed/27539197). J Am Med Inform Assoc. 2017 Apr 1;24(e1):e79-86. 

```{r}
text_df[1,'STUDY_NOTE_ID']
text_df[1, 'TEXT']
```

## CARD Resource Files
These files are inputs for the `.jar` invocation of CARD.

```{r}
word_map <- "code/card_inputs/word.txt" 
abbr_map <- "code/card_inputs/abbr.txt"
profile_dir <- "code/card_inputs/profile/" 
cui_map <- "code/card_inputs/VABBR_DS_beta.txt.add_semantic_type"
```

## Helper functions (subroutines)
These functions are used during the disambiguation process. 

** Note: ** using `system` presuposes that the system is compatible with the system listed in `sessionInfo()`. It should be noted that this command behaves differently on different OS. See docs for more information: [system documentation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/system.html). 


```{r}
execute_card <- function(row) {
  #' function to generate card output based on a clinical note piped from stdin
  #' @param row: a row in a dataframe conatined a char string that is a clinical note
  clinical_note_string <- as.String(row)
  command <- paste("java -jar code/TightSenseClustering.jar", word_map, abbr_map, profile_dir, cui_map)
  
  detected_abbreviations <- system(command,
                   input = clinical_note_string,
                   ignore.stdout=FALSE,
                   ignore.stderr=FALSE,
                   wait = FALSE,
                   intern = TRUE)
  
  result <- gsub("[\r\n]", "", detected_abbreviations)
  return(result) 
}


check_if_begin_sentence <- function(str_line) {
  #' boolean function returns 'T' iff str_line starts with 'Abbr:'
  #' @param str_line: a string value
  if ("BEGIN_SENTENCE:" %in% str_line) {
      return(TRUE)
  } else{
    return(FALSE) 
  } 
}

parse_abbr_line <- function(str_line) {
  #' function to parse a tab separated line and return detected and long-form abbreviations
  #' @param str_line: string to parse into tab-separated list
  parsed_line <- strsplit(str_line, '[\t]') [[1]]
  detected_text <- parsed_line[2] # index of detected text
  long_form_text <- parsed_line[4] # index of long-form text
  return(c(detected_text, long_form_text)) 
}


perform_substitions <- function(sentence, abbreviation_list, toPrint=TRUE) {
  #' function to return a new sentence with CARD predictions substituted
  #' @param sentence: a string containing the sentence with predicted abbreviations 
  #' @param abbreviation_list: list of detected abbrevations
  #' @param toPrint: boolean argument specifiying whether or not to print output to console
  new_sentence <- sentence
  
  #' @NOTE use first match (sub) to replace abbreviations in order
  
  for (abbr in abbreviation_list) {
      parsed_abbr <- parse_abbr_line(abbr)
      # note the asymetric whitespace around token to replace
      detected_abbr <- paste(parsed_abbr[1]," ", sep="")
      long_form_abbr <- paste(" ", parsed_abbr[2], " ", sep="")
    
      if (toPrint) {
        print(paste('detected:', detected_abbr, 'long form:', long_form_abbr)) 
      }
      
      new_sentence <- sub(detected_abbr, long_form_abbr, new_sentence)
  }
  
  if (toPrint) {
    print(paste('disambiguated sentence:', new_sentence)) 
  }
  return(new_sentence)
}
```

## Disambiguation

We pipe each record (clinical note) to a rewritten `MetaMapWrapper.SenseDisambiguationText` function. We execute the CARD java `.class` file via the `system()` command and, which expects a bash shell. Below is the flow control function including post-abbreviation processing. Here we manage the CARD output and perform disambiguation, outputting a new string with all sentences. Saved to new column of `data.frame`.

```{r}
disambiguate_note_contents <- function(row, toPrint=TRUE) {
  #' function to perform substitions for CARD abbreviation disambiguation
  #' @param row: row containing clinical note to disambiguate
  #' @param toPrint: boolean argument specifiying whether or not to print output to console, passed to substitution f(x)
  
  #' @NOTE generate CARD output for current row, returns a list()
  disambiguation_predictions <- execute_card(row)
  
  num_lines <- length(disambiguation_predictions)
  disamgiguated_note <- ""
  total_abbreviations <- 0

  #' @NOTE iterate through next lines holding position at a sentence
  #' @NOTE check if 'is sentence' by matching BEGIN_SENTENCE
  for (idx in 1:num_lines) {
      if (check_if_begin_sentence(disambiguation_predictions[[idx]]))  {
        # this will be the index and value of the sentence following the BEGIN_SENTENCE flag
        next_idx <- idx + 1
        current_sentence <- disambiguation_predictions[[next_idx]]

        # find next END_SENTENCE. flag
        num_indices_to_slice <- match('END_SENTENCE.', disambiguation_predictions[next_idx:num_lines])

        # start one beyond the sentence value (2 beyond the flag)
        start_slice <- next_idx + 1
        # end one short of the next END flag (do not include)
        end_slice <- idx + (num_indices_to_slice - 1)

        # get a slice with all abbreviations predicted for substituion
        if ((start_slice <= end_slice) & (start_slice < length(disambiguation_predictions))) {
          abbreviation_predictions <- disambiguation_predictions[start_slice:end_slice]
        } else{
          abbreviation_predictions <- NULL
        }

        if (toPrint) {
          print('####################################')
          print(paste('original sentence:', current_sentence))
        }

        num_abbreviations_detected <- 0

        # only call substitions on predicted abbrs
        if (!is.null(abbreviation_predictions)) {
          disambiguated_current_sentence <- perform_substitions(current_sentence, abbreviation_predictions, toPrint)
          disamgiguated_note <- paste(disamgiguated_note, disambiguated_current_sentence)
          num_abbreviations_detected <- length(abbreviation_predictions)
        } else {
          # always add sentence regardless if abbrs found
          disamgiguated_note <- paste(disamgiguated_note, current_sentence)
        } # end if abbreviation list NULL

        total_abbreviations <- total_abbreviations + num_abbreviations_detected

        if (toPrint) {
          print(paste('total abbreviations detected: ', num_abbreviations_detected))
        }
     } # end if current index is flag
  } # end for each index
  return(disamgiguated_note)
} 

# #TESTING
# sapply(text_df$TEXT[1:4], disambiguate_note_contents, toPrint=TRUE)


text_df$card_disambiguated_note <- sapply(text_df$TEXT, disambiguate_note_contents, toPrint=FALSE)
gc() # garbage collection
```

```{r}
head(text_df$card_disambiguated_note, 1)
```

We want to get an idea of how much CARD processes changed the raw notes. We use a few distance measures to look at this. We use the [restricted Damerau-Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance) method from the `stringdist` package.

```{r}
text_df$osa_distance_raw_to_CARD <- mapply(stringdist, a=text_df$TEXT, b=text_df$card_disambiguated_note, method=c('osa'))
head(text_df$osa_distance_raw_to_CARD)
```

```{r}
ggplot(text_df, aes(x=osa_distance_raw_to_CARD)) + 
  geom_histogram(aes(y=..density..), color="black", fill="lightblue") +
  geom_density(alpha=.2, fill="orange") +
  labs(title="Histogram of Optimal String Aligment Distance",x="OSA Distance (raw to CARD output)", y = "Density")
```


```{r}
# head(text_df, 4)
```


Sanity check. 

```{r}
nrow(text_df) # expect 104 on testing data, 1000 on sampled data
```


Write data out to file to inspect.

```{r}
# save_csv(text_df, 'data/', 'compare_disambiguation_results.csv')
```


# Text Mining

## Data Pre-processing
This section contains various transformation operations on the subsetted `data.frame`. The ultimate product is a new `data.frame` with increased dimensionality. The [DSPA Textbook (Chapter 19) provides additional TM/NLP rotocols](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/19_NLP_TextMining.html).


First we reaplce common contractions with their expanded forms. 
```{r}
fix_contractions <- function(doc){
  #' function adapted from 'https://www.datacamp.com/community/tutorials/R-nlp-machine-learning' to 
  #' replace common contractions 
  #' @param doc: input text field
  doc <- gsub("won't", "will not", doc)
  doc <- gsub("can't", "can not", doc)
  doc <- gsub("n't", " not", doc)
  doc <- gsub("'ll", " will", doc)
  doc <- gsub("'re", " are", doc)
  doc <- gsub("'ve", " have", doc)
  doc <- gsub("'m", " am", doc)
  doc <- gsub("'d", " would", doc)
  return(doc)
}


text_df$fixed_contractions <- sapply(text_df$card_disambiguated_note, fix_contractions)
```


This function performs a number of standard best-practice text preprocessing steps:

  - Load column vector containing text notes into library `tm` corpus object. Important to note that we create a new object, leaving the 'raw text notes' untouched during analysis. Importantly, here is the [documentation](https://www.rdocumentation.org/packages/tm/versions/0.7-6/topics/Corpus) for a corpus object. 
  - Remove punctuation characters and numbers and make all characters lowercase. **NOTE:** Numbers will be important for later stages of the analysis (for example, for extracting dosage information). These operations will need to be performed on the original text data.
  - `content_transformer(tolower)` responds to an update in `tm`. See: [/questions/24771165](https://stackoverflow.com/questions/24771165/r-project-no-applicable-method-for-meta-applied-to-an-object-of-class-charact).
  - Remove common English language words from the analysis. **Note on removing stopwords:**This is an important step to revisit, as many of these words may end up being quite useful.
  - Reduce each word to its 'stem,' or 'root.' See the [wikipedia page](https://en.wikipedia.org/wiki/Word_stem) for more information.
  - Tidy-up whitespace characters
  
  
```{r}
perform_text_preprocessing_operations <- function(columnVector, 
                                                  rmStopWords = FALSE, 
                                                  stemResults = FALSE) {
  #' function to chain together various text preprocessing functions
  #' @param columnVector: a column vector of textual data to process
  #' @param rmStopWords: boolean flag to determine if English stopwords should be removed
  #' @param stemResults: boolean flag to determine if words should be stemmed
  
  newVector <- columnVector
  doc_corpus <- tm::VCorpus(tm::VectorSource(newVector)) # create corpus opbject
  doc_corpus <- tm::tm_map(doc_corpus, tm::content_transformer(tolower)) # lowercase all
  doc_corpus <- tm::tm_map(doc_corpus, tm::removePunctuation) # remove punctuation characters
  doc_corpus <- tm::tm_map(doc_corpus, tm::removeNumbers) # remove numbers
  
  if (rmStopWords) {
    doc_corpus<-tm_map(doc_corpus, tm::removeWords, stopwords("english"))
  }
  
  if (stemResults) {
    doc_corpus<-tm_map(doc_corpus, tm::stemDocument)
  }
  
  doc_corpus <- tm_map(doc_corpus, tm::stripWhitespace) # clean up whitespace
  return(doc_corpus)
}


# invoke using defaults
doc_corpus <- perform_text_preprocessing_operations(text_df$fixed_contractions, 
                                                    rmStopWords = TRUE)
```

An empirical sanity-check to see what the precossed text looks like:

```{r}
doc_corpus[[2]]$content 
```

Add clean text back to `data.frame`. The `Corpus` object from `tm` is ordered, so we can apply this back into the subsetted `data.frame`.

```{r}
add_preprocessed_text_to_df <- function(df_to_append_to, cleaned_doc_corpus) {
  #' function to append a doc corpus back to a data frame as a new column, return modified input data.frame
  #' @param df_to_append_to: data.frame to append new column to
  #' @param cleaned_doc_corpus: processed doc corpus object
  
  temp.df <- data.frame(text=unlist(sapply(cleaned_doc_corpus, `[`, "content")), 
    stringsAsFactors=F)

  df_to_append_to$processed_text <- temp.df$text
  return(df_to_append_to)
}

text_df <- add_preprocessed_text_to_df(text_df, doc_corpus)
# head(text_df)
```

Let's again look at the OSA distances

```{r}
text_df$osa_distance_processed_to_CARD <- mapply(stringdist, a=text_df$processed_text, b=text_df$card_disambiguated_note, method=c('osa'))
head(text_df$osa_distance_processed_to_CARD)
```

```{r}
ggplot(text_df, aes(x=osa_distance_processed_to_CARD)) + 
  geom_histogram(aes(y=..density..), color="black", fill="lightblue") +
  geom_density(alpha=.2, fill="orange") +
  labs(title="Histogram of Optimal String Aligment Distance",x="OSA Distance (processed to CARD output)", y = "Density")
```


```{r}
text_df$osa_distance_processed_to_raw <- mapply(stringdist, a=text_df$processed_text, b=text_df$TEXT, method=c('osa'))
head(text_df$osa_distance_processed_to_raw)
```


```{r}
ggplot(text_df, aes(x=osa_distance_processed_to_raw)) + 
  geom_histogram(aes(y=..density..), color="black", fill="lightblue") +
  geom_density(alpha=.2, fill="orange") +
  labs(title="Histogram of Optimal String Aligment Distance",x="OSA Distance (raw to processed)", y = "Density")
```


We can save the results to disk for empirical validation and inspection by uncommenting this following block.

```{r}
# save_csv(text_df, 'data/', 'compare_disambiguation_and_preprocessing.csv.csv')
```


## TF-IDF

For a primer, please read the [wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Note the high sparsity. This first function is a 'stub' that may be used to manually limit the vocabulary used to commpute TF-IDF (for example, a controlled medical vocabulary with synonyms...). For now we rely on TF-IDF to parse all unique tokens in the input space. We will see how this scales.

```{r}
# get_dictionary_from_file<- function(filepath){
#   #' function to return a vector of words to be used in a TF-IDF matrix
#   #' @param filepath: path of file containing lexical resources
#   #' @NOTE: stub
# }

```

The function below builds a TF-IDF matrix from the corpus object created in the preprocessing steps above.

```{r}
build_tfidf_matrix <- function(text_vector, ID_vector, dictionary) {
  #' fuction to return a new tf_idf object
  #' @param corpus: a corpus object to use to create the tf_idf mat
  #' @param ID_vector: vector of document IDs to add to tf_idf matrix
  #' @param dictionary: a corpus of words to use instead of those passed from the corpus
  
  corpus <- tm::VCorpus(tm::VectorSource(text_vector))
  
  if (missing(dictionary)) {
    dtm.tfidf <- tm::DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
    dtm.tfidf$dimnames$Docs <- as.character(ID_vector)
    return(dtm.tfidf)
  } else {
   print(paste0('NEED! Dictionary procedure. ', dictionary)) 
  }
}

dtm.tfidf <- build_tfidf_matrix(text_vector = text_df$processed_text, ID_vector = text_df$STUDY_NOTE_ID)

# this test is for when a dictionary-based approach can be taken (depending on the stub above)
# build_tfidf_matrix(corpus = doc_corpus,
#                                 dictionary = 'test',
#                                 ID_vector = text_df$STUDY_NOTE_ID)
```

Inspect the TF_IDF matrix, note the high sparsity.

```{r}
inspect(dtm.tfidf)
```


Remove sparse terms, more bang for our buck.

```{r}
dtm.tfidf <- tm::removeSparseTerms(dtm.tfidf, sparse = .95)
```

Convert tf_idf object to `data.frame` and pivot so that each term is a column. Important reference: [questions/8161836](https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-an-r-dataframe).

```{r}
transpose_tf_idf_mat <- function(tf_idf_mat) {
  #' function to pivot matrix so that it may be joined to the original matrix as new colum vectors
  #' @param tf_idf_mat: tf idf matrix to trabnsform
  
  dtm.tfidf_long <- tidy(tf_idf_mat, row_names=dtm.tfidf$dimnames$Docs, col_names=dtm.tfidf$dimnames$Terms)
  
  # avoid possible conflicts with the word 'document'
  colnames(dtm.tfidf_long)[colnames(dtm.tfidf_long)=="document"] <- 'document_id'
  
  dtm.tfidf_df <- dcast(dtm.tfidf_long, document_id ~ term , value.var = 'count' )
  # replace uncomputed values with 0
  dtm.tfidf_df <- dtm.tfidf_df %>% replace(is.na(.), 0) # this is slow
  return(dtm.tfidf_df)
}

dtm.tfidf_df <- transpose_tf_idf_mat(dtm.tfidf)
```


```{r}
find_duplicate_columns <- function(tf_idf_dataframe) {
  #' function to print/remove duplicate columns
  #' @param tf_idf_dataframe: data.frame to check for duplicate columns
  #' @param remove_dupes: boolean flag. If TRUE the duplicare columns will be stripped
  
  # duplicates <- tf_idf_dataframe[, !duplicated(colnames(tf_idf_dataframe))]
  duplicates <- colnames(tf_idf_dataframe)[duplicated(colnames(tf_idf_dataframe))]
  
  
  print(paste('total unique columns in tf_idf:', length(unique(names(tf_idf_dataframe)))))
  print(paste('total columns in tf_idf:', length(names(tf_idf_dataframe))))
  print(paste('duplicate columns in tf_idf:', unlist(duplicates)))
}

find_duplicate_columns(dtm.tfidf_df)
```

Add term counts back to original `data.frame` as new variables.

```{r}
new_features_df <- merge(x = text_df,
                         y = dtm.tfidf_df,
                         by.x=c('STUDY_NOTE_ID'),
                         by.y=c('document_id'),
                         all.x = TRUE)
# head(new_features_df)
```

Sanity checks

```{r}
ncol(new_features_df) # expect many
```


```{r}
nrow(new_features_df) # expect 1000
```


Save the resulting file (note that there are other methods for doing this).

```{r}
# save_csv(new_features_df, 'data/', 'new_feature_extraction.csv')
```

# Exploratory Textual Analysis

## Tokenization

This section is based loosely on [Lyric Analysis with NLP & Machine Learning with R](https://www.datacamp.com/community/tutorials/R-nlp-machine-learning):

  _R NLP & Machine Learning: Lyric Analysis [Internet]. DataCamp Community. 2018 [cited 2019 Jul 16]. Available from: https://www.datacamp.com/community/tutorials/R-nlp-machine-learning_
  
```{r}
unnested_token_df <-  text_df %>%
  unnest_tokens(word, processed_text) %>%
  anti_join(stop_words) %>%
  distinct() 

# head(unnested_token_df)
```

```{r}
names(text_df)
```

The following block is taken from [https://www.datacamp.com/community/tutorials/R-nlp-machine-learning](https://www.datacamp.com/community/tutorials/R-nlp-machine-learning) and produces a nice graphic.

```{r}
full_word_count <- text_df %>%
  unnest_tokens(word, processed_text) %>%
  group_by(AUTHOR_SERVICE) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words)) 

full_word_count[1:20,] %>%
  ungroup(num_words, AUTHOR_SERVICE) %>%
  mutate(num_words = formattable::color_bar("lightblue")(num_words)) %>%
  mutate(AUTHOR_SERVICE = color_tile("orange","orange")(AUTHOR_SERVICE)) %>%
  kable("html", escape = FALSE, align = "c", caption = "Departments with the highest number of words (verbosity)") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)
```

```{r}
# full_word_count %>%
#   ggplot() +
#     geom_histogram(aes(x = num_words)) +
#     ylab("Note Count") + 
#     xlab("Word Count per Note") +
#     ggtitle("Word Count Distribution") +
#     theme(plot.title = element_text(hjust = 0.5),
#           legend.title = element_blank(),
#           panel.grid.minor.y = element_blank())
```



```{r}
full_word_count <- text_df %>%
  unnest_tokens(word, processed_text) %>%
  group_by(NOTE_TYPE) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words)) 

full_word_count[1:20,] %>%
  ungroup(num_words, NOTE_TYPE) %>%
  mutate(num_words = formattable::color_bar("lightblue")(num_words)) %>%
  mutate(NOTE_TYPE = color_tile("orange","orange")(NOTE_TYPE)) %>%
  kable("html", escape = FALSE, align = "c", caption = "Departments with the highest number of words (verbosity)") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)
```


```{r}
unnested_token_df %>%
  count(word, sort = TRUE) %>%
  top_n(30) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
    geom_col(aes(word, n), fill = "lightblue") +
    theme(legend.position = "none", 
          plot.title = element_text(hjust = 0.5),
          panel.grid.major = element_blank()) +
    xlab("") + 
    ylab("Note Count") +
    ggtitle("Most Frequently Used Words in Notes Sample") +
    coord_flip()
```

```{r}
words_counts <- unnested_token_df %>%
  count(word, sort = TRUE) 

wordcloud2(words_counts[1:300, ], size = .5)
```

Words by Author Services:

```{r}
words_by_author <- unnested_token_df %>% 
  group_by(AUTHOR_SERVICE) %>%
  count(word, AUTHOR_SERVICE, sort = TRUE) %>%
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(AUTHOR_SERVICE,n) %>%
  mutate(row = row_number()) 

# note, this will need to be handled differently as the input space scales
words_by_author[1:50,] %>%
  ggplot(aes(row, n, fill = AUTHOR_SERVICE)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "Note Count") +
    ggtitle("Popular Words by Author Service") + 
    facet_wrap(~AUTHOR_SERVICE, scales = "free") +
    scale_x_continuous(  # This handles replacement of row 
      breaks = words_by_author$row, # notice need to reuse data frame
      labels = words_by_author$word) +
    coord_flip()
```

```{r}
words_by_type <- unnested_token_df %>% 
  group_by(NOTE_TYPE) %>%
  count(word, NOTE_TYPE, sort = TRUE) %>%
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(NOTE_TYPE,n) %>%
  mutate(row = row_number()) 

# note, this will need to be handled differently as the input space scales
words_by_type[1:50,] %>%
  ggplot(aes(row, n, fill = NOTE_TYPE)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "Note Count") +
    ggtitle("Popular Words by Author Service") + 
    facet_wrap(~NOTE_TYPE, scales = "free") +
    scale_x_continuous(  # This handles replacement of row 
      breaks = words_by_type$row, # notice need to reuse data frame
      labels = words_by_type$word) +
    coord_flip()
```


```{r}
#unnest and remove undesirable words, but leave in stop and short words
word_lengths <- text_df %>%
  unnest_tokens(word, processed_text) %>%
  distinct() %>%
  mutate(word_length = nchar(word)) 

word_lengths %>%
  count(word_length, sort = TRUE) %>%
  ggplot(aes(word_length), 
         binwidth = 10) + 
    geom_histogram(aes(fill = ..count..),
                   breaks = seq(1,25, by = 2), 
                   show.legend = FALSE) + 
    xlab("Word Length") + 
    ylab("Word Count") +
    ggtitle("Word Length Distribution") +
    theme(plot.title = element_text(hjust = 0.5),
          panel.grid.minor = element_blank())
```

```{r}
# head(text_df)
```




```{r}
lex_diversity_per_type <- text_df %>%
  filter(NOTE_TYPE != "NA") %>%
  unnest_tokens(word, processed_text) %>%
  group_by(NOTE_TYPE) %>%
  summarise(lex_diversity = n_distinct(word)) %>%
  arrange(desc(lex_diversity)) 

diversity_plot <- lex_diversity_per_type %>%
  ggplot(aes(NOTE_TYPE, lex_diversity)) +
    geom_point(color = 'blue',
               alpha = .4, 
               size = 4, 
               position = "jitter") + 
    stat_smooth(color = "black", se = FALSE, method = "lm") +
    geom_smooth(aes(x = NOTE_TYPE, y = lex_diversity), se = FALSE,
                color = "blue", lwd = 2) +
    ggtitle("Lexical Diversity") +
    xlab("") + 
    ylab("") +
    scale_color_manual(values = brewer.pal(11, "Spectral")) +
    theme_classic() +
    coord_flip()

diversity_plot
```

```{r}
lex_density_per_type <- text_df %>%
  filter(NOTE_TYPE != "NA") %>%
  unnest_tokens(word, processed_text) %>%
  group_by(NOTE_TYPE) %>%
  summarise(lex_density = n_distinct(word)/n()) %>%
  arrange(desc(lex_density))

density_plot <- lex_density_per_type %>%
  ggplot(aes(NOTE_TYPE, lex_density)) + 
    geom_point(color = "blue",
               alpha = .4, 
               size = 4, 
               position = "jitter") + 
    stat_smooth(color = "black", 
                se = FALSE, 
                method = "lm") +
    geom_smooth(aes(x = NOTE_TYPE, y = lex_density), 
                se = FALSE,
                color = "blue", 
                lwd = 2) +
    ggtitle("Lexical Density") + 
    xlab("") + 
    ylab("") +
    scale_color_manual(values = brewer.pal(11, "Spectral")) +
    theme_classic() +
    coord_flip()

density_plot
```


## N-Gram Analysis

```{r}
notes_ngrams <- tidytext::unnest_tokens(text_df, ngram, 'processed_text', token = "ngrams", n = 3)

# head(notes_ngrams)
```

```{r}
notes_ngrams %>%
  count(ngram, sort = TRUE)
```


```{r}
ngram_tf_idf <- notes_ngrams %>%
  count(STUDY_NOTE_ID, ngram) %>%
  bind_tf_idf(ngram, STUDY_NOTE_ID, n) %>%
  arrange(desc(tf_idf))

ngram_tf_idf
```

Sample a few terms:

```{r}
sample(names(new_features_df[,7:length(names(new_features_df))]), 100)
```


## Inspecting the TF_IDF matrix

```{r}
tm::findFreqTerms(dtm.tfidf, lowfreq = 1)
```

```{r}
tm::findAssocs(dtm.tfidf, "symptom", corlimit = 0.2)
```

```{r}
tm::findAssocs(dtm.tfidf, "dementia", corlimit = 0.1)
```

## Term Frequencies

Below we define a function to calculate raw counts and relative frequecies from a given column of textual data. 

```{r}
names(text_df)
```


```{r}
get_token_frequecy_from_column <- function(df, columnName, sortByHighFQ=TRUE) {
  #' function to return a new data.frame w/ term frequencies from a specified column
  #' @param df: data.frame containing column to use
  #' @param columnName: colName of the column to use; expects a string value not index position
  tmp_corpus <- VCorpus(VectorSource(df[, columnName]))
  tdm <- as.matrix(TermDocumentMatrix(tmp_corpus))
  frequency_df <- data.frame(token = rownames(tdm), 
                      count = rowSums(tdm), 
                      row.names = NULL)
  
  total_count <- sum(frequency_df$count)
  frequency_df$normalized_frequency <- sapply(frequency_df$count, function(x) x/total_count)
  
  
  if (!sortByHighFQ) {
    return(frequency_df)
  } else {
    # sort output by most frequent tokens
    frequency_df <- frequency_df[with(frequency_df,order(-count)),]  
  }
  return(frequency_df)
}

raw_frequencies <- get_token_frequecy_from_column(text_df, 'processed_text', sortByHighFQ=TRUE)
# head(raw_frequencies, 50)
```

If we need to save a relative frequency file for CARD for any reason, here is a function to do it:

```{r}
create_term_frequency_file_for_card <- function(term_frequency_df) {
  #' function to save a file with term frequencies in CARD format:
  #' each line formated as 'word \t normalized_frequency'.(Normalized frequency = (word frequency)/(Total # of words))
  #' @param term_frequency_df: data.frame containing word frequency information
  today_date = format(Sys.time(), "%m%d%Y")
  filename = "word_frequencies.csv" # note, this needs to be changed
  save_path = paste("data/", today_date, filename , sep="")

  write.csv(term_frequency_df[, c('token', 'normalized_frequency')], 
            file=save_path, 
            sep = "\t", 
            row.names=FALSE,
            col.names=FALSE)
  
  print(paste('File:', save_path, 'written successfully!'))
}
  
# # uncomment to create file
# create_term_frequency_file_for_card(raw_frequencies)
```


## Remove Sparse Terms

This will allow us to remove rarely occurring words and reduce the dimensionality of the dataset, should we need to do this.

```{r}
# doc_dtm_spase <-removeSparseTerms(doc_dtm, 0.90)
# doc_dtm
# doc_dtm_spase
```


/////////////////////////////////////////////////////////////////////////////////////////////////
# Appendix: Older Code

## Abbreviations

Detect Abbreviations using pretreined SVM, save results to new directory.

```{r}
# run_CARD_abbrev_jar <- function(jar_path, input_dir, output_dir) {
#   # function to run CARD abbreviation detection
#   # @param jar_path: path to the executable .jar file
#   # @param input_dir: directory with clinical notes
#   # @param output_dir: directory to save CARD output
#   
#   command <- paste("java -jar ", jar_path," -i ", input_dir, " -o ", output_dir, " -sb", sep="")
#   system(command, intern = TRUE)
#   
# }
# 
# 
# run_CARD_abbrev_jar(jar_path = 'CARD_dataset_tools/AbbreviationDetection_v1.1_beta.jar-02-19/AbbreviationDetection_v1.1_beta.jar',
#                                 input_dir = 'text_files_for_CARD/',
#                                 output_dir = 'abbreviation_output_CARD/')
```


Running CARD via classpath (not via jar)

```{r}
# execute_card <- function(row) {
#   # 'function to generate card output based on a clinical note piped from stdin
#   # '@row: a row in a dataframe conatined a char string that is a clinical note
#   clinical_note_string <- as.String(row)
#   command <- "java -classpath ./bin MetaMapWrapper.SenseDisambiguationText"
#   
#   detected_abbreviations <- system(command,
#                    input = clinical_note_string,
#                    ignore.stdout=FALSE,
#                    ignore.stderr=FALSE,
#                    wait = FALSE,
#                    intern = TRUE)
#   
#   result <- gsub("[\r\n]", "", detected_abbreviations)
#   return(result) 
# }
```

## Sense Detection

```{r}
### IMPORTANT NOTE: this function REQUIRES being run from the given path as wd, or else CARD can't find the classes on classpath
# set execution path for this chunk

# exe_path <- 'TightClusteringSenseDetection_v2' # required to run 'MetaMapWrapper.SenseDisambiguationText'
# setwd(exe_path) 
# getwd()


# run_CARD_sense_jar <- function(jar_path, lang_file, input_dir, output_dir) {
#   # function to run CARD abbreviation detection
#   # @param jar_path: path to the executable .jar file
#   # @param input_dir: directory with clinical notes
#   # @param output_dir: directory to save CARD output
#   
#   command <- paste("java -jar ", jar_path," -i ", input_dir, " -o ", output_dir, " -av ", lang_file, sep="")
#   system(command, intern = TRUE)
#   
# }
# 
# # run sense detection using adam
# run_CARD_sense_jar(jar_path = 'CARD_dataset_tools/SenseDetection_v1.0_beta/SenseDetection_v1.0_beta.jar',
#                                 lang_file = 'CARD_dataset_tools/adam_database.txt',
#                                 input_dir = 'text_files_for_CARD/',
#                                 output_dir = 'abbreviation_output_CARD/')
```

Saving dataframes to disk

```{r}
# save_whole_df_file <- function(df, filename) {
#   # function to save data.frame to a .csv file with each record in the data.frame
#   # saved as record in csv
#   # @param df A data.frame object to be saved
#   # @param filename A string value to which the current date is appended in order to save the file
# 
#   today_date = format(Sys.time(), "%m%d%Y")
#   card_input_dir = 'text_files_for_CARD/'
#   save_path = paste(card_input_dir, today_date, filename , sep="") # append today's date
#   write.csv(df,save_path, row.names = FALSE)
#   print(paste('DataFrame saved to file saved to ', save_path))
# 
# }
# 
# save_whole_df_to_card_file(text_df, 'card_input.csv')
```


**WARNING:** This cell will cause problems if run as-is for large data.frames. The following function will save each record to a unique file for CARD. 

```{r}
# ##### SAVE EACH RECORD AS NEW FILE
# save_each_record_to_card_file <- function(df) {
#   # function to save data.frame to a .csv file with each record in the data.frame
#   # saved as record in csv
#   # @param df A data.frame object to be saved, each row will be saved to unique file
# 
#   for (i in 1:nrow(df)) {
#     clincial_note <- df[i,'TEXT'] #  variable for clinical note value
#     # creates a unique ID string to be used as filename
#     card_input_dir = 'text_files_for_CARD/'
#     id_string <- paste(df[i,'STUDY_PAT_ID'], df[i,'STUDY_ENC_ID'], df[i,'STUDY_NOTE_ID'], sep="-")
#     save_path = paste(card_input_dir, id_string, ".txt" , sep="")
# 
#     if (file.exists(save_path)) {
#             print(paste("WARNING:'", normalizePath(save_path), "'already exists. File not saved. Please check your work."))
#       } else {
#             write.csv(clincial_note, file=save_path, row.names = FALSE)
#             print(paste('Note ID saved:', df[i,'STUDY_NOTE_ID']))
#       }
#   }
# }
# 
# save_each_record_to_card_file(text_df)
```

## Subset Text Field
Subset the `data.frame`. We retain the text notes and ID fields for mapping back to the data.frame.

```{r}
# subset_dataframe <- function(my_dataframe, column_vector) {
#   #' function to subset a dataframe based on list of columns, returns new data.frame
#   #' @param my_dataframe: a data.frame from which to derive the subset
#   #' @param column_vector: list of strings containing columns to included in new data.frame
#   subset.df <- subset(my_dataframe, select=column_vector)
#   return(subset.df)
# }
# 
# # list of columns to process
# cols = c('STUDY_PAT_ID', 'STUDY_ENC_ID', 'STUDY_NOTE_ID','TEXT')
# text_df <- subset_dataframe(transformed_df, cols) # note, working with sampled DF
# 
# head(text_df , 2)
```

## Term-document matrix code

```{r}
# doc_dtm <- tm::DocumentTermMatrix(doc_corpus)
# doc_dtm$dimnames$Docs <- as.character(text_df$STUDY_NOTE_ID) # make sure to preserve document names
```


```{r}
# get_shortest_document_length <- function(doc_dtm) {
#   #' function to find the shortest document
#   #' @param corpusObject
#   rowTotals <- apply(doc_dtm , 1, sum)
#   print(min(rowTotals)) # the shortest document
# }
# 
# get_shortest_document_length(doc_dtm)
```






<!--html_preserve-->
<div>
    	<footer><center>
			<a href="http://www.socr.umich.edu/">SOCR Resource</a>
				Visitor number <img src="http://counter.digits.net/?counter=SOCR"
	 			align="middle" border="0" height="20" hspace="4" vspace="2" width="60">
				<script type="text/javascript">
					var d = new Date();
					document.write(" | " + d.getFullYear() + " | ");
				</script> 
				<a href="http://socr.umich.edu/img/SOCR_Email.png"><img alt="SOCR Email"
	 			title="SOCR Email" src="http://socr.umich.edu/img/SOCR_Email.png"
	 			style="border: 0px solid ;"></a>
	 		 </center>
	 	</footer>

	<!-- Start of StatCounter Code -->
		<script type="text/javascript">
			var sc_project=5714596; 
			var sc_invisible=1; 
			var sc_partition=71; 
			var sc_click_stat=1; 
			var sc_security="038e9ac4"; 
		</script>
		
		<script type="text/javascript" src="https://www.statcounter.com/counter/counter.js"></script>
	<!-- End of StatCounter Code -->
	
	<!-- GoogleAnalytics -->
		<script src="https://www.google-analytics.com/urchin.js" type="text/javascript"> </script>
		<script type="text/javascript"> _uacct = "UA-676559-1"; urchinTracker(); </script>
	<!-- End of GoogleAnalytics Code -->
</div>
<!--/html_preserve-->
