---
title: "Health Analytics Collaboratory (HAC) - University of Michigan"
subtitle: "<h2><u>HAC Alzheimer's Disease Digital Health Analytics Project: Text Preprocessing</u></h2>"
author: "<h3>Ivo Dinov, Cooper Stansbury, Haiyin Liu, Bingxin Chen</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
tags: [HAC, DSPA, SOCR, MIDAS, Big Data, Predictive Analytics, Computable Phenotypes] 
output:
  html_document:
    theme: spacelab
    highlight: tango
    includes:
      before_body: SOCR_header.html
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

# Processing Logistics

## Note on Project Directory  Structure:
This project assumes that:
  - There is a root working directory (in my case, called 'HAC')
  - There is a subdirectory called `data/` with the data files (`.xlsx`).
  - `CARD_dataset_tools/` has been downloaded to the working directory.
  - `.Rmd` scripts are in a directory called something like `notebooks/`.
 
## Working Citations
  - [Text_Processing_In_R](http://www.mjdenny.com/Text_Processing_In_R.html)
  - [r-text-mining-pre-processing](https://analytics4all.org/2016/12/22/r-text-mining-pre-processing/)
  - [SICSS_Basic_Text_Analysis](https://cbail.github.io/SICSS_Basic_Text_Analysis.html)
  - [19_NLP_TextMining](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/19_NLP_TextMining.html)

## Packages/Dependencies

```{r}
library(readxl)
library(tidyverse)
library(tm)
library(SnowballC)
library(tidytext)
library(tidyr)
library(dplyr)
library(rJava)

## another text_processing library that may be worth considering, spaCy is extremely powerful
## https://github.com/quanteda/spacyr
# library(quanteda)
```

Selected Package Documentation Links
  - [library(tm)](https://cran.r-project.org/web/packages/tm/tm.pdf)
  - [library(SnowballC)](https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf)
  - [library(tidytext)](https://cran.r-project.org/web/packages/tidytext/tidytext.pdf)

```{r}
sessionInfo()
```

```{r}
# expect 1.0.1, 0.7.0, 1.0.1
sapply(c('repr', 'IRdisplay', 'IRkernel'), function(p) paste(packageVersion(p)))
```

# Load data
Load the data from the source file. Assumes file in `.xlsx` format. 

```{r}
getwd() # make sure you're in the right spot! Expect "/Users/milk/Desktop/work/HAC"
```

This assumes that there is a separate directory for the data file. **Watch out for paths.**

```{r}
read_data_to_dataframe <- function(filepath) {
  # open .xlsx file into r data.frame, returns new data.frame
  # @param filepath: a string with the relative path for the file to be opened
  
  df <- as.data.frame(read_excel(filepath))
  print(paste('Input object has dimensionality:', toString(dim(df))))
  print(paste('Input object n rows:', toString(nrow(df))))
  print(paste('Input object n columns:', toString(ncol(df))))
  
  return(df)
}


raw_data.df <- read_data_to_dataframe('data/DS_PreprocessedAnonymized_Text.xlsx')
```

# Subset Text Field
Subset the `data.frame`. We retain the text notes and ID fields for mapping back to the data.frame.

```{r}
subset_dataframe <- function(my_dataframe, column_vector) {
  # function to subset a dataframe based on list of columns, returns new data.frame
  # @param my_dataframe: a data.frame from which to derive the subset
  # @param column_vector: list of strings containing columns to included in new data.frame
  
  subset.df <- subset(my_dataframe, select=column_vector)
  return(subset.df)
  
}

# list of columns to process
cols = c('STUDY_PAT_ID', 'STUDY_ENC_ID', 'STUDY_NOTE_ID','DS_PreprocessedAnonymized_Text')
text.df <- subset_dataframe(raw_data.df, cols)

head(text.df , 2)
```

# Abbreviation Disambiguiation

We utilize on [CARD](https://sbmi.uth.edu/ccb/resources/abbreviation.htm):
  - Wu Y, Denny JC, Trent Rosenbloom S, Miller RA, Giuse DA, Wang L, et al. [A long journey to short abbreviations: developing an open-source framework for clinical abbreviation recognition and disambiguation (CARD)](https://www.ncbi.nlm.nih.gov/pubmed/27539197). J Am Med Inform Assoc. 2017 Apr 1;24(e1):e79-86. 
  
## Save raw_text for CARD

At this time we save all raw clinical notes to a separate file for ingestion into CARD for acronym disambiguation. CARD runs as compiled Java code and expects each note in a separate file, so we may need to change this procedure.

```{r}
# save_whole_df_file <- function(df, filename) {
#   # function to save data.frame to a .csv file with each record in the data.frame
#   # saved as record in csv
#   # @param df A data.frame object to be saved
#   # @param filename A string value to which the current date is appended in order to save the file
# 
#   today_date = format(Sys.time(), "%m%d%Y")
#   card_input_dir = 'text_files_for_CARD/'
#   save_path = paste(card_input_dir, today_date, filename , sep="") # append today's date
#   write.csv(df,save_path, row.names = FALSE)
#   print(paste('DataFrame saved to file saved to ', save_path))
# 
# }
# 
# save_whole_df_to_card_file(text.df, 'card_input.csv')
```

Below we inspect the size of the largest character string in the data and extrapolate an overly pessimistic worst-case scenario. 

```{r}
largest_clinical_note <- max(nchar(text.df$DS_PreprocessedAnonymized_Text, type = "bytes"))

memory_req_estimate_bt <- largest_clinical_note * length(text.df)
memory_req_estimate_gb <- memory_req_estimate_bt * 0.000000001
print(paste('Naive (pessimistic) disk space estimate (bytes):',  memory_req_estimate_bt))
print(paste('Naive (pessimistic) disk space estimate (gb):',  memory_req_estimate_gb))
print(paste('Naive (pessimistic) disk space estimate for all file I/O (gb):',  2.2*memory_req_estimate_gb))
```

**WARNING:** This cell will cause problems if run as-is for large data.frames. The following function will save each record to a unique file for CARD. 

```{r}
##### SAVE EACH RECORD AS NEW FILE
save_each_record_to_card_file <- function(df) {
  # function to save data.frame to a .csv file with each record in the data.frame
  # saved as record in csv
  # @param df A data.frame object to be saved, each row will be saved to unique file

  for (i in 1:nrow(df)) {
    clincial_note <- df[i,'DS_PreprocessedAnonymized_Text'] #  variable for clinical note value
    # creates a unique ID string to be used as filename
    card_input_dir = 'text_files_for_CARD/'
    id_string <- paste(df[i,'STUDY_PAT_ID'], df[i,'STUDY_ENC_ID'], df[i,'STUDY_NOTE_ID'], sep="-")
    save_path = paste(card_input_dir, id_string, ".txt" , sep="")

    if (file.exists(save_path)) {
            print(paste("WARNING:'", normalizePath(save_path), "'already exists. File not saved. Please check your work."))
      } else {
            write.csv(clincial_note, file=save_path, row.names = FALSE)
            print(paste('Note ID saved:', df[i,'STUDY_NOTE_ID']))
      }
  }
}

save_each_record_to_card_file(text.df)
```

## Abbreviations

Detect Abbreviations using pretrqined SVM, save results to new directory.

```{r}
# run_CARD_abbrev_jar <- function(jar_path, input_dir, output_dir) {
#   # function to run CARD abbreviation detection
#   # @param jar_path: path to the executable .jar file
#   # @param input_dir: directory with clinical notes
#   # @param output_dir: directory to save CARD output
#   
#   command <- paste("java -jar ", jar_path," -i ", input_dir, " -o ", output_dir, " -sb", sep="")
#   system(command, intern = TRUE)
#   
# }
# 
# 
# run_CARD_abbrev_jar(jar_path = 'CARD_dataset_tools/AbbreviationDetection_v1.1_beta.jar-02-19/AbbreviationDetection_v1.1_beta.jar',
#                                 input_dir = 'text_files_for_CARD/',
#                                 output_dir = 'abbreviation_output_CARD/')
```

## Sense Detection

```{r}
# run_CARD_sense_jar <- function(jar_path, lang_file, input_dir, output_dir) {
#   # function to run CARD abbreviation detection
#   # @param jar_path: path to the executable .jar file
#   # @param input_dir: directory with clinical notes
#   # @param output_dir: directory to save CARD output
#   
#   command <- paste("java -jar ", jar_path," -i ", input_dir, " -o ", output_dir, " -av ", lang_file, sep="")
#   system(command, intern = TRUE)
#   
# }
# 
# # run sense detection using adam
# run_CARD_sense_jar(jar_path = 'CARD_dataset_tools/SenseDetection_v1.0_beta/SenseDetection_v1.0_beta.jar',
#                                 lang_file = 'CARD_dataset_tools/adam_database.txt',
#                                 input_dir = 'text_files_for_CARD/',
#                                 output_dir = 'abbreviation_output_CARD/')
```


## Disambiguiation

Here we call the `disambiguiate` function from the `.jar` file: `CARD_dataset_tools/disambiguation/TightClusteringSenseDetection`. This function expects each clinical note will be in it's own `.txt` file, a requirement that doesn't suite our application. Our strategy is to replace all detected abbreviations.

Note on execution from the command-line (from the `readme.txt`: `CARD_dataset_tools/disambiguation/README.txt`).

  - `cd TightClusteringSenseDetection/`
  - Generate CARD independent output: `java -cp ./bin MetaMapWrapper.SenseDisambiguationText input_dir card_output_dir`
  - Where, input_dir contains all the input txt files. The CARD independent result will be generated under 'card_output_dir'.


Note: using `system` presuposes that the system is compatible with the system listed in `sessionInfo()`. It should be noted that this command behaves differently on different OS. See docs for more information: [system documentation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/system.html). 
```{r}
get_CARD_abbreviations <- function(set_path, classpath, input_dir, output_dir) {
  # function to find acronyms based on abbreviation detection
  # @param set_path: 'MetaMapWrapper.SenseDisambiguationText' expects to be run from the root directory, set_path
  # should be the root directory for the MetaMapWrapper Class
  # @param classpath: the class path for MetaMapWrapper, './bin' per readme, though it could be different
  # @param input_dir: expected input is the abbreviation output directory, though the raw text files can be
  # used as well
  # @param output_dir: directory to store detected acroynyms and possible disambiguations
  
  # this is a workaround to the classpath invocation expected by CARD, may be hard to reproduce on different os
  setwd(set_path) 
  
  # java -cp "./bin" "MetaMapWrapper.SenseDisambiguationText" ${CARD_INPUT_DIR} ${CARD_OUTPUT_DIR} 
  command <- paste("java -cp ", classpath,' "MetaMapWrapper.SenseDisambiguationText" ', input_dir, " ", output_dir, sep="")
  print(command)
  system(command, intern = TRUE)
  
}


# NOTE: this command takes as input clinical notes in separate files (will not scale)
# NOTE: this function is run on the RAW text files, rather than the abbreviation output
get_CARD_abbreviations(set_path = 'CARD_dataset_tools/disambiguation/TightClusteringSenseDetection/',
                  classpath = './bin',
                  input_dir = '../../../text_files_for_CARD/',
                  output_dir = '../../../disambiguation_output_CARD/')


# get_CARD_abbreviations(set_path = 'CARD_dataset_tools/disambiguation/TightClusteringSenseDetection/',
#                   classpath = './bin',
#                   input_dir = '../../../abbreviation_output_CARD/sent',
#                   output_dir = '../../../disambiguation_output_CARD/')

```

**NOTE:** The chunk type has switched to `bash`. We will execute a number of shell commands before returning to R.

We want to make sure that there are no collisions, expect 104 records in dir
```{bash}
ls -1 text_files_for_CARD/ | wc -l
```

Count the output files.
```{bash}
ls -1 disambiguation_output_CARD/ | wc -l
```

# Post-hoc Acronym Processing
Here we manage the CARD output and perform disambiguation, outputting a new string with all sentences with their respective IDs.

```{r}
check_if_abbreviation <- function(str_line) {
  # boolean function returns 'T' iff str_line starts with 'Abbr:'
  # @param str_line: a string value
  if (startsWith(str_line, "Abbr:")) {
      return(TRUE)
  } else{
    return(FALSE)
  }

}


parse_abbr_line <- function(str_line) {
  # function to parse a tab separated line and return detected and long-form abbreviations
  # @param str_line: string to parse into tab-separated list
  parsed_line <- strsplit(str_line, '[\t]') [[1]]
  detected_text <- parsed_line[2] # index of detected text
  long_form_text <- parsed_line[4] # index of long-form text
  return(c(detected_text, long_form_text))
}


clean_sentence <- function(str_line) {
  # function to return a sentence without the 'Sentence=' string
  # @param str_line: expects sentence only input
  return(gsub("Sentence=", "", str_line))
}


parse_id <- function(filepath) {
  # function to recover the ID from a string 
  # @param filepath: filepath containing the ID for the clinical note
  
  strip_extension <- gsub(".txt", "", filepath)
  parsed_line <- strsplit(strip_extension, '[-]')
  return(parsed_line)
}


disambiguate_files <- function(directory) {
  # function to replace CARD predictions and return a new datadframe
  # @param directory: A string representation of a directory path containing ...
  # ... the output files of the CARD disambiguation process

  file_list <- list.files(directory, pattern = "*.txt")
  
  # data.frame to store the results
  df <- data.frame('STUDY_PAT_ID' = character(0),
                   'STUDY_ENC_ID'  = character(0), 
                   'STUDY_NOTE_ID' = character(0),
                   'disambiguated_text' = character(0),
                   stringsAsFactors = FALSE)
  

  for (filepath in file_list) {
          relative_path <- paste(directory, filepath, sep="") # make sure program is aware of relative path
          con <- file(relative_path, "r") # open file connection
          print(paste('processing:', filepath))
          file_contents <- scan(relative_path, what="", sep="\n") # get all non-blank lines using scan()
          num_lines <- length(file_contents) # how long is the file?
          
          # empty string used to paste disambiguated results
          disamgiguated_sentences <- "" 

          # loop through each line of the file to perform disambiguiations
          # note the starting index=2, to skip over the 'x' in the first line
          # created by row.names=FALSE in the file write proc
          for (idx in 2:num_lines) {
            
              current_line <- file_contents[[idx]]
              
              # is current line a 'sentence?'
              if (!check_if_abbreviation(current_line)) {
                  is_current_sent <- 1 # line is 'sentence'
                  cleaned_sentence <- clean_sentence(current_line) # clean the sentence of 'Sentence='
              } else {
                  is_current_sent <- 0 # line is abbreviation
              }
              
              # iterate through next lines holding position at a sentence, taking care not to run off end of file
              if ((idx < num_lines) & (is_current_sent == 1))  {
                
                    next_idx <- idx + 1
                
                      
                    ########## TODO #########
                    # this function currently replaces duplicate abbreviations 
                    # (if the abbreviation is used twice)
                    # twice, we still require two passes
                    # good area for future optimization
                    
                    # while next line is an abbreviation
                    while ((next_idx < num_lines) & (check_if_abbreviation(file_contents[[next_idx]]))) {
                      
                            next_line <- file_contents[[next_idx]]
                      
                            parsed_abbreviation <- parse_abbr_line(next_line)
                            detected_abbr <- parsed_abbreviation[1]
                            long_form_abbr <- parsed_abbreviation[2]
                            
                            cleaned_sentence <- gsub(detected_abbr, long_form_abbr, cleaned_sentence)
                            
                            # print(paste("### FOUND: ###", detected_abbr, "### IN: ###", cleaned_sentence))
                            # print(" ")
                            next_idx = next_idx + 1
                    }
                
                # add disambiguated sentence back into string variable
                disamgiguated_sentences <- paste(disamgiguated_sentences, cleaned_sentence)
              }
          } 
      # parse the enbcounter ID back from the filename
      id_vec <- parse_id(filepath = filepath) [[1]]
      new_row <- c(id_vec, disamgiguated_sentences)
      
      # append disambiguated text into data.frame
      df[nrow(df) + 1,] <- new_row 
      close(con) # close file connection
  }
  
  return(df)
}


disambiguate_df <- disambiguate_files('disambiguation_output_CARD/')
```

Sanity checks
```{r}
nrow(disambiguate_df) # expect 104 on testing data
```

```{r}
head(disambiguate_df)
```

### Merge Disambiguated Text Back in

We merge the output of the CARD process with the raw text data for the next pre-processing steps.

```{r}
text.df <- merge(x = text.df, y = disambiguate_df,
                 by=c("STUDY_PAT_ID","STUDY_ENC_ID", "STUDY_NOTE_ID"), all = TRUE)
head(text.df)

```

```{r}
text.df[2,'disambiguated_text']
```


```{r}
nrow(text.df) # expect 104
```

# Text Mining Pre-processing

This section contains various transformation operations on the subsetted `data.frame`. The ultimate product is a new `data.frame` with increased dimensionality. The [DSPA Textbook (Chapter 19) provides additional TM/NLP rotocols](http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/19_NLP_TextMining.html).

## Transform Clinical Notes into VCorpus
Load column vector containing text notes into library `tm` corpus object. Important to note that we create a new object, leaving the 'raw text notes' untouched during analysis. Importantly, here is the [documentation](https://www.rdocumentation.org/packages/tm/versions/0.7-6/topics/Corpus) for a corpus object.


```{r}
doc_corpus <- VCorpus(VectorSource(text.df[,'disambiguated_text']))
doc_corpus
doc_corpus[[2]]$content # print out the first example 
```

## Remove Punctuation and Numbers

We remove punctuation characters and numbers and make all characters lowercase. **NOTE:** Numbers will be important for later stages of the analysis (for example, for extracting dosage information). These operations will need to be performed on the original text data.

**Note:** `content_transformer(tolower)` responds to an update in `tm`. See: [/questions/24771165](https://stackoverflow.com/questions/24771165/r-project-no-applicable-method-for-meta-applied-to-an-object-of-class-charact).


'|NAME|' is a derivative product of the de-identification process. While it may be important for NER-type tasks downstream, we can rely on the original text field for this information. 

```{r}
doc_corpus <- tm_map(doc_corpus, removeWords, c('|NAME|'))
doc_corpus<-tm_map(doc_corpus, content_transformer(tolower)) # lowercase all, this makes disambiguation easier
doc_corpus<-tm_map(doc_corpus, removePunctuation) # remove punctuation characters
doc_corpus<-tm_map(doc_corpus, removeNumbers) # remove numbers
doc_corpus[[2]]$content # empirical sanity-check
```

## Removing Stopwords

We remove common English language words from the analysis. **NOTE:** This is an important step to revisit, as many of these words may end up being quite useful.

```{r}
stopwords("english") # set the dictionary to use
```

Let's now actually remove the stopwords.

```{r}
doc_corpus<-tm_map(doc_corpus, removeWords, stopwords("english"))
doc_corpus[[2]]$content # another empirical sanity-check
```

## Stemming

Reduce each word to its 'stem,' or 'root.' See the [wikipedia page](https://en.wikipedia.org/wiki/Word_stem) for more information.

```{r}
doc_corpus<-tm_map(doc_corpus, stemDocument)
doc_corpus[[2]]$content # sanity-check
```

## Clean-Up Whitespaces

```{r}
doc_corpus <- tm_map(doc_corpus, stripWhitespace) # normalize whitespace
doc_corpus[[2]]$content # sanity-check
```

## Add clean text back to `data.frame`

The `Corpus` object from `tm` is ordered, so we can apply this back into the subsetted `data.frame`.

```{r}
temp.df <- data.frame(text=unlist(sapply(doc_corpus, `[`, "content")), 
    stringsAsFactors=F)

text.df$processed_text <- temp.df$text
head(text.df)
```

Here we build a single ID column. (may move this higher up in the notebook).

```{r}
text.df$full_id <- paste(text.df$STUDY_PAT_ID, text.df$STUDY_ENC_ID, text.df$STUDY_NOTE_ID, sep="-")
head(text.df)
```

################## TODO ################## 
  1. Resolve issues leading to empty post-processing notes.
################## TODO ################## 

Here's  problematic case example

```{r}
text.df[22, c('DS_PreprocessedAnonymized_Text', 'disambiguated_text', 'processed_text')]
```



# Term Frequency, Inverse Document Frequency (TF-IDF)

For a primer, please read the [wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). 

```{r}
doc_dtm<-DocumentTermMatrix(doc_corpus)
doc_dtm
```

Get word counts for each document (first 'record' is a matrix ID, not a count).

```{r}
rowTotals <- apply(doc_dtm , 1, sum)
rowTotals
```

Name each document.

```{r}

doc_dtm$dimnames$Docs <- as.character(1:nrow(text.df))
inspect(doc_dtm)
```


```{r}
findFreqTerms(doc_dtm, lowfreq = 25) # look at terms that appear at least 25 times
```

# Create tf-idf object 

```{r}
dtm.tfidf <- DocumentTermMatrix(doc_corpus, control = list(weighting=weightTfIdf))
dtm.tfidf
```

```{r}
# dtm.tfidf$dimnames$Docs <- text.df$full_id
# inspect(dtm.tfidf[1:5, 1:2])
```


## Convert tf object to `data.frame` and pivot so that each term is a column. Important reference: [questions/8161836](https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-an-r-dataframe).


########################################### TODO #############################################
  - function not joining correctly, need to revisit
  - might be a good candidate for a function
########################################### TODO #############################################


```{r}
doc_dtm.df <- tidy(doc_dtm) # convert to data.frame
terms_as_columns.df <- spread(doc_dtm.df, key = 'term', value = 'count') # pivot data.frame
terms_as_columns.df <- terms_as_columns.df %>% replace(is.na(.), 0) # replace NA with 0 in a way that will scale
head(terms_as_columns.df)
```

## Add term counts back to original `data.frame` as new variable.

```{r}
text_tfidf.df <- cbind(text.df, terms_as_columns.df)
head(text_tfidf.df)
```

```{r}
ncol(text_tfidf.df)
```


```{r}
text_tfidf.df[2, 'DS_PreprocessedAnonymized_Text']
```


## Quick Descriptive Peek

Here we look in the document term matrix for positive correlation with the word stem `memori`. See anything interesting?

```{r}
findAssocs(doc_dtm, "memori", corlimit = 0.9)
```

## Remove Sparse Terms

This will allow us to remove rarely occurring words and reduce the dimensionality of the dataset, should we need to do this.

```{r}
# doc_dtm_spase <-removeSparseTerms(doc_dtm, 0.90)
# doc_dtm
# doc_dtm_spase
```


/////////////////////////////////////////////////////////////////////////////////////////////////
# Appendix: Older Code

Attempts at writing a I/O wrapper for Java Proc. The goal is to see if it's possible to stream data directly from R instead of writing files in order to manage larger amounts of data. Currently, these procs do not work.

```{r}
# .jinit()
# print(paste('classpath:', .jclassPath()))
# .jaddClassPath(dir('CARD_dataset_tools/disambiguation/TightClusteringSenseDetection/bin/', full.names=TRUE))
# print(paste('classpath:', .jclassPath()))

# output_dir <- .jnew("java/lang/String", 'disambiguation_output_CARD/')
# input_dir <- .jnew("java/lang/String", 'text_files_for_CARD/')
# str_Array <- .jarray(c(input_dir,output_dir))
# evalARR <- .jevalArray(str_Array)
# 
# disambiguator_class <- .jnew("MetaMapWrapper.SenseDisambiguationText", evalARR)
# 
# .jmethods(disambiguator_class)
# .jconstructors(disambiguator_class)

# J(disambiguator_class, 'main', input_dir)

# test_asSTR <- as.String(text.df[2, 'DS_PreprocessedAnonymized_Text'])
# note_as_java_string <- .jnew("java/lang/String", test_asSTR)

# print(.jevalArray(str_Array))

# test <- J(disambiguator_java_class, method='parse_sentence', note_as_java_string)
# test <- J(disambiguator_java_class, method='main', .jevalArray(str_Array))

# test <- .jcall(disambiguator_java_class, returnSig="V",
#              method='main',
#              .jevalArray(str_Array))
# 
# print(test)


# Sorry for answering an old question, but this has bugged me as well for some time. The answer is: ;
# The format of type specification for non-primitive return types is Lpackage/subpackage/Type; - it has to end with a semicolon. So in the # example above, you would need:

# out = .jcall(jobject,"Ljava/util/List;","lookup",input)

# out <- .jcall(disambiguator_java_class, returnSig="V",
#              method='main',
#              note_as_java_string, output_dir)

# # works!
# out <- .jcall(disambiguator_java_class, returnSig='Ljava/lang/String;', 
#              method='parse_sentence',
#              note_as_java_string)

# out <- J(disambiguator_java_class, method='parse_sentence', note_as_java_string) # works

# print(out)


# for (i in 1:2) {
#   clinical_note <- text.df[i,'DS_PreprocessedAnonymized_Text']
  # print(clinical_note)}
```

Outdated, but potentially useful examples of grep and CARD predictions.

```{bash}
################################# Run CARD predictions on exported data ################################# 
# CARD_PATH='CARD_dataset_tools/AbbreviationDetection_v1.1_beta.jar-02-19/AbbreviationDetection_v1.1_beta.jar'
# CARD_INPUT_DIR='text_files_for_CARD/'
# CARD_OUTPUT_DIR='abbreviation_output_CARD/'

# java -jar ${CARD_PATH} -i ${CARD_INPUT_DIR} -o ${CARD_OUTPUT_DIR}
```

```{bash}
################################# Run CARD Disambiguation on RAW data ################################# 
# java -version

# change the working dir
# cd "CARD_dataset_tools/disambiguation/TightClusteringSenseDetection" 
# note that these locations are rather aribrary, this just needs to be the output dir with raw text
# CARD_INPUT_DIR='../../../text_files_for_CARD/' 
# CARD_OUTPUT_DIR='../../../disambiguation_output_CARD'

# java -cp "./bin" "MetaMapWrapper.SenseDisambiguationText" ${CARD_INPUT_DIR} ${CARD_OUTPUT_DIR} 

# make sure we 'reset' our working directory to keep things tidy
# cd '../../../' 
pwd # sanity check
```


```{bash}
################################# grep exact matches (brute force approach) ################################# 
### Use `grep` to find exact (`-w`) matches. Note `>` overwrites an existing file with this name. **WARNING:** This will not scale.

# CARD_PREDICTIONS='abbreviation_output_CARD/CARD_predict_abbrs.txt'
# LRABR_LEXICON='CARD_dataset_tools/LRABR.txt'
# ADAM_LEIXCON='CARD_dataset_tools/adam_database.txt'
# NEW_FILE='data/possible_matches_test.txt'

# # need to specify a lexical resource, not sure which is better
# grep -w -f ${CARD_PREDICTIONS} ${ADAM_LEIXCON} > ${NEW_FILE}
# grep -w -f ${CARD_PREDICTIONS} ${LRABR_LEXICON} > ${NEW_FILE}

# head -n 10 ${NEW_FILE}
```


```{bash}
################################# Count  possible matches ################################# 
### Count possible matches
# NEW_FILE='data/possible_matches_test.txt'
# wc -l ${NEW_FILE}
```



```{bash}
################################# Load exact matches ################################# 
# This cell block reads `data/possible_matches_test.txt`, which is the result of the bash block `grep` above. This block assumes that the resource used # was the `adam_database.txt`. This loading block is specific to that file.

# From the documentation:

#  - Column 1: Preferred abbreviation (the most commonly used)                    
#  - Column 2: Variants of the abbreviation                                       
#  - Column 3: Long-form and variants (for each variant, the count and its score are given)                                           
#  - Column 4: Score (the likelihood of the long-form being a phrase)             
#  - Column 5: Count (the number of times the abbreivation/long-form pair has been defined)  
```


```{r}
### NOTE: this is an R function in a bash cell. 


# filepath='data/possible_matches_test.txt'
# 
# acronyms.df <- read.table(filepath, 
#                           sep = '\t',
#                           header = FALSE, 
#                           fill = TRUE, 
#                           quote = "")
# 
# # reset column names to make them easier to read
# names(acronyms.df) <- c("preferred_abbreviation", "variants", "variants_lf", "score", "count")
# head(acronyms.df)
```


<!--html_preserve DO NOT CHANGE THIS FOOTER -->
<div>
    	<footer><center>
			<a href="http://www.socr.umich.edu/">SOCR Resource</a>
				Visitor number <img src="http://counter.digits.net/?counter=SOCR"
	 			align="middle" border="0" height="20" hspace="4" vspace="2" width="60">
				<script type="text/javascript">
					var d = new Date();
					document.write(" | " + d.getFullYear() + " | ");
				</script> 
				<a href="http://socr.umich.edu/img/SOCR_Email.png"><img alt="SOCR Email"
	 			title="SOCR Email" src="http://socr.umich.edu/img/SOCR_Email.png"
	 			style="border: 0px solid ;"></a>
	 		 </center>
	 	</footer>

	<!-- Start of StatCounter Code -->
		<script type="text/javascript">
			var sc_project=5714596; 
			var sc_invisible=1; 
			var sc_partition=71; 
			var sc_click_stat=1; 
			var sc_security="038e9ac4"; 
		</script>
		
		<script type="text/javascript" src="https://www.statcounter.com/counter/counter.js"></script>
	<!-- End of StatCounter Code -->
	
	<!-- GoogleAnalytics -->
		<script src="https://www.google-analytics.com/urchin.js" type="text/javascript"> </script>
		<script type="text/javascript"> _uacct = "UA-676559-1"; urchinTracker(); </script>
	<!-- End of GoogleAnalytics Code -->
</div>
<!--/html_preserve-->
